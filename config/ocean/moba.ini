[base]
package = ocean
env_name = moba
policy_name = MOBA
rnn_name = Recurrent

[train]
total_timesteps = 200_000_000
checkpoint_interval = 50
learning_rate = 0.0005884593954100382
num_envs = 2
num_workers = 2
env_batch_size = 1
update_epochs = 1 #4
gamma = 0.97122709010199
gae_lambda = 0.9950791242953836
clip_coef = 0.25287469618744873
vf_clip_coef = 0.0031348221071211702
vf_coef = 0.4894856074992443
ent_coef = 0.00010185603917634786
max_grad_norm = 0.661034882068634
bptt_horizon = 16
#batch_size = 1_048_576
#minibatch_size = 32_768
batch_size = 256000
minibatch_size = 16000
compile = False
anneal_lr = False
device = cuda

[env]
reward_death = -0.4490417540073395
reward_xp = 0.0016926873475313188
reward_distance = 0.0
reward_tower = 4.525112152099609
num_envs = 400

[sweep.metric]
goal = maximize
name = environment/elo

[sweep.parameters.env.parameters.reward_death]
distribution = uniform
min = -5.0
max = 0

[sweep.parameters.env.parameters.reward_xp]
distribution = uniform
min = 0.0
max = 0.05

[sweep.parameters.env.parameters.reward_distance]
distribution = uniform
min = 0.0
max = 0.5

[sweep.parameters.env.parameters.reward_tower]
distribution = uniform
min = 0.0
max = 5.0
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 200_000_000
max = 200_000_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 512000
max = 2048000

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 16000
max = 128000

